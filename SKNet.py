import torch
from torch import nn

# thop ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ FLOPs(ì—°ì‚°ëŸ‰)ì™€ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ import
from thop import profile
from thop import clever_format

# SKConv í´ë˜ìŠ¤ ì •ì˜: Selective Kernel Convolution
class SKConv(nn.Module):
    def __init__(self, features, M=2, G=32, r=16, stride=1, L=32):
        """
        Args:
            features: ì…ë ¥ ì±„ë„ ìˆ˜ (number of input channels).
            M: ë¸Œëœì¹˜(branch)ì˜ ê°œìˆ˜. ê¸°ë³¸ê°’ì€ 2.
            G: ê·¸ë£¹ ì»¨ë³¼ë£¨ì…˜ì—ì„œ ê·¸ë£¹ì˜ ìˆ˜.
            r: z ë²¡í„°ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë¹„ìœ¨.
            stride: ìŠ¤íŠ¸ë¼ì´ë“œ(Stride). ê¸°ë³¸ê°’ì€ 1.
            L: z ë²¡í„°ì˜ ìµœì†Œ ê¸¸ì´. ê¸°ë³¸ê°’ì€ 32.
        """
        super(SKConv, self).__init__()
        # z ë²¡í„°ì˜ ê¸¸ì´ ê³„ì‚°. ìµœì†Œê°’ Lì„ ë³´ì¥.
        d = max(int(features / r), L)
        self.M = M  # ë¸Œëœì¹˜ ìˆ˜
        self.features = features  # ì…ë ¥ ì±„ë„ ìˆ˜
        
        # ë¸Œëœì¹˜ë³„ë¡œ ì„œë¡œ ë‹¤ë¥¸ ì»¤ë„ í¬ê¸°ë¥¼ ê°€ì§„ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ìƒì„±
        self.convs = nn.ModuleList([])
        for i in range(M):
            self.convs.append(nn.Sequential(
                nn.Conv2d(features, features, kernel_size=3, stride=stride, padding=1+i, dilation=1+i, groups=G, bias=False),
                nn.BatchNorm2d(features),
                nn.ReLU(inplace=True)
            ))
        
        # GAP(Global Average Pooling) ë ˆì´ì–´
        self.gap = nn.AdaptiveAvgPool2d((1, 1))
        
        # z ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” FC ë ˆì´ì–´
        self.fc = nn.Sequential(
            nn.Conv2d(features, d, kernel_size=1, stride=1, bias=False),
            nn.BatchNorm2d(d),
            nn.ReLU(inplace=True)
        )
        
        # ê° ë¸Œëœì¹˜ì— ëŒ€í•œ attention vectorë¥¼ ê³„ì‚°í•˜ëŠ” FC ë ˆì´ì–´
        self.fcs = nn.ModuleList([])
        for i in range(M):
            self.fcs.append(
                nn.Conv2d(d, features, kernel_size=1, stride=1)
            )
        
        # Softmax í•¨ìˆ˜ë¡œ attention weight ê³„ì‚°
        self.softmax = nn.Softmax(dim=1)
        
    def forward(self, x):
            batch_size = x.shape[0]

            # ê° ë¶„ê¸°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
            feats = [conv(x) for conv in self.convs]
            
            feats = torch.cat(feats, dim=1)  # ë¶„ê¸° ê²°ê³¼ë¥¼ ì±„ë„ ë°©í–¥ìœ¼ë¡œ ì—°ê²°
            # torch.catì„ í†µí•´ ê° ë¶„ê¸°ì˜ ì¤‘ìš”ë„ ë²¡í„°ë¥¼ ì±„ë„ ë°©í–¥( dim = 1 )ìœ¼ë¡œ ê²°í•©í•©ë‹ˆë‹¤. 
            # ([8, 256, 56, 56])

            feats = feats.view(batch_size, self.M, self.features, feats.shape[2], feats.shape[3])
            # ê²°í•©ëœ í…ì„œë¥¼ [ ğµ , ğ‘€ Ã— ğ¶ , 1 , 1 ] ì—ì„œ [ ğµ , ğ‘€ , ğ¶ , 1 , 1 ] ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤
            # ([8, 2, 128, 56, 56])  
            # ([8, [0], 128, 56, 56]) / ([8, [1], 128, 56, 56])
            # ë°°ì¹˜,ë¶„ê¸°,ì…ë ¥ì±„ë„,ë¶„ê¸°í›„ í”¼ì³ í¬ê¸°    
            

            # ëª¨ë“  ë¶„ê¸°ì˜ íŠ¹ì§•ì„ í•©ì‚°í•˜ì—¬ U ìƒì„± 
            feats_U = torch.sum(feats, dim=1)
            # ([8, 128, 56, 56])


            # Global Average Poolingìœ¼ë¡œ S ìƒì„±(Fgp)
            feats_S = self.gap(feats_U)
            # torch.Size([8, 128, 1, 1])

            # ì±„ë„ ì¶•ì†Œ ë° í™œì„±í™”í•˜ì—¬ Z ìƒì„±(Ffc)
            feats_Z = self.fc(feats_S)
            # torch.Size([8, 32, 1, 1])

            # ê° ë¶„ê¸°ì˜ ì¤‘ìš”ë„ ê³„ì‚°
            attention_vectors = [fc(feats_Z) for fc in self.fcs] 
            # A(cz), B(cz) : torch.Size([8,Â 128,Â 1,Â 1])
            attention_vectors = torch.cat(attention_vectors, dim=1)
            # AB : torch.Size([8,Â 256,Â 1,Â 1])
            attention_vectors = attention_vectors.view(batch_size, self.M, self.features, 1, 1)
            # A / B : torch.Size([8, 2 , 128,Â Â 1,Â 1])
            attention_vectors = self.softmax(attention_vectors)
            # ac / bc : torch.Size([8, 2 , 128,Â Â 1,Â 1])

            # ì¤‘ìš”ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° ë¶„ê¸°ì˜ íŠ¹ì§• ê²°í•©
            feats_V = torch.sum(feats * attention_vectors, dim=1)
            return feats_V
    
# SKNetì˜ ê¸°ë³¸ ë‹¨ìœ„ì¸ SKUnit ì •ì˜
class SKUnit(nn.Module):
    def __init__(self, in_features, mid_features, out_features, M=2, G=32, r=16, stride=1, L=32):
        """
        Args:
            in_features: ì…ë ¥ ì±„ë„ ìˆ˜.
            mid_features: ì¤‘ê°„ ì±„ë„ ìˆ˜.
            out_features: ì¶œë ¥ ì±„ë„ ìˆ˜.
            M: ë¸Œëœì¹˜ ìˆ˜.
            G: ê·¸ë£¹ ìˆ˜.
            r: z ë²¡í„°ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë¹„ìœ¨.
            stride: ìŠ¤íŠ¸ë¼ì´ë“œ.
            L: z ë²¡í„°ì˜ ìµœì†Œ ê¸¸ì´.
        """
        super(SKUnit, self).__init__()
        
        # ì²« ë²ˆì§¸ 1x1 ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_features, mid_features, 1, stride=1, bias=False),
            nn.BatchNorm2d(mid_features),
            nn.ReLU(inplace=True)
        )
        
        # SKConv ë ˆì´ì–´
        self.conv2_sk = SKConv(mid_features, M=M, G=G, r=r, stride=stride, L=L)
        
        # ë‘ ë²ˆì§¸ 1x1 ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv3 = nn.Sequential(
            nn.Conv2d(mid_features, out_features, 1, stride=1, bias=False),
            nn.BatchNorm2d(out_features)
        )
        
        # Shortcut ì„¤ì •
        if in_features == out_features:
            # ì…ë ¥ ì°¨ì›ê³¼ ì¶œë ¥ ì°¨ì›ì´ ë™ì¼í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            self.shortcut = nn.Sequential()
        else:
            # ì…ë ¥ ì°¨ì›ê³¼ ì¶œë ¥ ì°¨ì›ì´ ë‹¤ë¥´ë©´ ì°¨ì›ì„ ë§ì¶”ê¸° ìœ„í•´ 1x1 ì»¨ë³¼ë£¨ì…˜ ì¶”ê°€
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_features, out_features, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_features)
            )
        
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        residual = x  # ì…ë ¥ì„ ì €ì¥ (shortcutì— ì‚¬ìš©)
        
        # ë©”ì¸ ê²½ë¡œ
        out = self.conv1(x)
        out = self.conv2_sk(out)
        out = self.conv3(out)
        
        # Shortcut ì—°ê²°ê³¼ í•©ì‚°
        return self.relu(out + self.shortcut(residual))


# SKNet ì •ì˜
class SKNet(nn.Module):
    def __init__(self, class_num, nums_block_list=[3, 4, 6, 3], strides_list=[1, 2, 2, 2]):
        """
        Args:
            class_num: ë¶„ë¥˜ í´ë˜ìŠ¤ ê°œìˆ˜.
            nums_block_list: ê° stageì—ì„œ ì‚¬ìš©í•  ë¸”ë¡ ìˆ˜.
            strides_list: ê° stageì—ì„œì˜ stride ê°’.
        """
        super(SKNet, self).__init__()
        # ê¸°ë³¸ Conv ë ˆì´ì–´
        self.basic_conv = nn.Sequential(
            nn.Conv2d(3, 64, 7, 2, 3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )
        
        # Max Pooling ë ˆì´ì–´
        self.maxpool = nn.MaxPool2d(3, 2, 1)
        
        # ë„¤ ê°€ì§€ stageë¥¼ ì •ì˜
        self.stage_1 = self._make_layer(64, 128, 256, nums_block=nums_block_list[0], stride=strides_list[0])
        self.stage_2 = self._make_layer(256, 256, 512, nums_block=nums_block_list[1], stride=strides_list[1])
        self.stage_3 = self._make_layer(512, 512, 1024, nums_block=nums_block_list[2], stride=strides_list[2])
        self.stage_4 = self._make_layer(1024, 1024, 2048, nums_block=nums_block_list[3], stride=strides_list[3])
        
        # Global Average Pooling
        self.gap = nn.AdaptiveAvgPool2d((1, 1))
        # Fully Connected Layer
        self.classifier = nn.Linear(2048, class_num)
        
    def _make_layer(self, in_feats, mid_feats, out_feats, nums_block, stride=1):
        # ì²« ë¸”ë¡ì€ strideë¥¼ ì ìš©
        layers = [SKUnit(in_feats, mid_feats, out_feats, stride=stride)]
        # ë‚˜ë¨¸ì§€ ë¸”ë¡ì€ stride ì—†ì´ ì¶”ê°€
        for _ in range(1, nums_block):
            layers.append(SKUnit(out_feats, mid_feats, out_feats))
        return nn.Sequential(*layers)

    def forward(self, x):
        fea = self.basic_conv(x)
        fea = self.maxpool(fea)
        fea = self.stage_1(fea)
        fea = self.stage_2(fea)
        fea = self.stage_3(fea)
        fea = self.stage_4(fea)
        fea = self.gap(fea)
        fea = torch.squeeze(fea)
        fea = self.classifier(fea)
        return fea


# SKNet ë²„ì „ ì •ì˜
def SKNet26(nums_class=1000):
    return SKNet(nums_class, [2, 2, 2, 2])

def SKNet50(nums_class=1000):
    return SKNet(nums_class, [3, 4, 6, 3])

def SKNet101(nums_class=1000):
    return SKNet(nums_class, [3, 4, 23, 3])


# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == '__main__':
    # ì…ë ¥ ë°ì´í„° ìƒì„± (8ê°œì˜ ë°°ì¹˜, 3ì±„ë„, 224x224 ì´ë¯¸ì§€)
    x = torch.rand(8, 3, 224, 224)
    
    # SKNet26 ëª¨ë¸ ìƒì„±
    model = SKNet26()
    
    # ëª¨ë¸ì„ í†µí•´ ì¶œë ¥ ê³„ì‚°
    out = model(x)
    
    # FLOPsì™€ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    flops, params = profile(model, (x, ))
    # ê³„ì‚° ê²°ê³¼ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…
    flops, params = clever_format([flops, params], "%.5f")
    
    # ê²°ê³¼ ì¶œë ¥
    print(flops, params)
    print('out shape : {}'.format(out.shape))
